{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import torch\n",
    "from utils.data import build_dataset,build_xview_dataset, unwrap_collate_fn\n",
    "from attrdict import AttrDict\n",
    "from utils.group_by_aspect_ratio import create_aspect_ratio_groups, GroupedBatchSampler\n",
    "from utils.fcos import fcos_resnet50_fpn\n",
    "# from torchvision.models.detection import fcos_resnet50_fpn\n",
    "from torchvision.models.detection import ssd300_vgg16\n",
    "\n",
    "import datetime\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "from utils.engine import train_and_eval,eval_model\n",
    "import torchvision\n",
    "from pycocotools import mask as coco_mask\n",
    "from pycocotools.coco import COCO\n",
    "import math\n",
    "from lr_schedulers import WarmupWrapper\n",
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "\n",
    "from utils.model import make_custom_object_detection_model_fcos, build_frcnn_model\n",
    "import matplotlib.pyplot as plt\n",
    "from train import load_dataset\n",
    "import numpy as np\n",
    "\n",
    "from determined.experimental import Determined\n",
    "from PIL import Image, ImageDraw\n",
    "# remove warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from collections import OrderedDict\n",
    "from sahi import AutoDetectionModel\n",
    "from sahi.predict import get_sliced_prediction, predict, get_prediction\n",
    "from sahi.utils.file import download_from_url\n",
    "from sahi.utils.cv import read_image\n",
    "from IPython.display import Image as Imagey\n",
    "# Set up .detignore file so the checkpoints directory is not packaged into future experiments\n",
    "!echo checkpoints > .detignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_exp(lr=None,momentum=None,epochs=None):\n",
    "    '''\n",
    "    '''\n",
    "    model = build_frcnn_model(dataset.num_classes)\n",
    "    model.to(device)\n",
    "\n",
    "    optimizer = torch.optim.SGD(\n",
    "            model.parameters(),\n",
    "            lr=lr,\n",
    "            momentum=momentum,\n",
    "            weight_decay=1e-4,\n",
    "            nesterov=\"nesterov\",\n",
    "        )\n",
    "\n",
    "    scheduler_cls = WarmupWrapper(MultiStepLR)\n",
    "    scheduler = scheduler_cls(\n",
    "        'linear',  # warmup schedule\n",
    "        100,  # warmup_iters\n",
    "        0.001,  # warmup_ratio\n",
    "        optimizer,\n",
    "        [177429, 236572],  # milestones\n",
    "        0.1,  # gamma\n",
    "    )\n",
    "    print(\"Start training\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    losses, model = train_and_eval(model,data_loader,data_loader_test,optimizer,scheduler,device,cpu_device,epochs=epochs)\n",
    "\n",
    "    total_time = time.time() - start_time\n",
    "    total_time_str = str(datetime.timedelta(seconds=int(total_time)))\n",
    "    print(f\"Training time {total_time_str}\")\n",
    "\n",
    "def load_determined_state_dict(ckpt):\n",
    "    '''\n",
    "    Removes module from state dict keys as determined saves model in DataParallel format:\n",
    "    https://discuss.pytorch.org/t/solved-keyerror-unexpected-key-module-encoder-embedding-weight-in-state-dict/1686/4\n",
    "    '''\n",
    "    new_state_dict = OrderedDict()\n",
    "    for k, v in ckpt['models_state_dict'][0].items():\n",
    "        name = k[7:] # remove `module.`\n",
    "        new_state_dict[name] = v\n",
    "    return new_state_dict\n",
    "def visualize_pred(inv_tensor,res,targets_t):\n",
    "    '''\n",
    "    '''\n",
    "    img = Image.fromarray((255.*inv_tensor.cpu().permute((1,2,0)).numpy()).astype(np.uint8))\n",
    "    draw = ImageDraw.Draw(img)\n",
    "    # draw ground truth\n",
    "    print(\"Num GT Boxes: \",targets_t[0]['boxes'].shape[0])\n",
    "    for ind,(b,l) in enumerate(zip(targets_t[0]['boxes'],targets_t[0]['labels'])):\n",
    "        # print(b.detach().numpy(), s.detach().numpy())\n",
    "        x,y,x2,y2 = b.detach().numpy()\n",
    "        # print( x,y,x2,y2,l.item())\n",
    "        draw.rectangle([x,y,x2,y2],fill=None,outline=(0,255,0))\n",
    "        draw.text([x,y-10],\"{}\".format(l),fill=None,outline=(0,255,0))\n",
    "\n",
    "    idx = list(res.keys())[0]\n",
    "    print(\"Num Pred Boxes: \",res[idx]['boxes'].shape[0])\n",
    "    for ind,(b,s,l) in enumerate(zip(res[idx]['boxes'],res[idx]['scores'],res[idx]['labels'])):\n",
    "        # print(b.detach().numpy(), s.detach().numpy())\n",
    "        x,y,x2,y2 = b.detach().numpy()\n",
    "        # print( x,y,x2,y2,s.item(),l.item())\n",
    "        draw.rectangle([x,y,x2,y2],fill=None,outline=(255,0,0))\n",
    "        draw.text([x,y-10],\"{}\".format(l),fill=None,outline=(255,0,0))\n",
    "\n",
    "\n",
    "\n",
    "    plt.imshow(img)\n",
    "def predict(model,images_t,targets_t):\n",
    "    '''\n",
    "    '''\n",
    "    cpu_device = torch.device('cpu')\n",
    "    device = torch.device('cuda')\n",
    "    images_t = list(image.to(device) for image in images_t)\n",
    "    outputs = model(images_t)\n",
    "    # print(x,outputs)\n",
    "    outputss = []\n",
    "    for t in outputs:\n",
    "        outputss.append({k: v.to(cpu_device) for k, v in t.items()})\n",
    "    # model_time = time.time() - model_time\n",
    "    res = {target[\"image_id\"].item(): output for target, output in zip(targets_t, outputss)}\n",
    "    visualize_pred(images_t[0],res,targets_t)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/determined-ai/determined/master/determined-logo.png\" align='right' width=150 />\n",
    "\n",
    "# Building a Geospatial Detection Model with Determined\n",
    "\n",
    "<img src=\"https://www.cis.upenn.edu/~jshi/ped_html/images/PennPed00071_1.png\" width=400 />\n",
    "\n",
    "\n",
    "This notebook will walk through the benefits of building a Deep Learning model with Determined.  We will build an object detection model trained on the [Penn-Fudan Database for Pedestrian Detection and Segmentation](https://www.cis.upenn.edu/~jshi/ped_html/).\n",
    "\n",
    "\n",
    "# Table of Contents\n",
    "\n",
    "\n",
    "<font size=\"3\">\n",
    "<ol>\n",
    "  <li>What Modeling looks like Today</li>\n",
    "  <li>Building a model with Determined\n",
    "    <ol>\n",
    "      <li>Single GPU training</li>\n",
    "      <li>Cluster-scale multi-GPU training</li>\n",
    "      <li>Adapative hyperparameter search</li>\n",
    "    </ol>\n",
    "  </li>\n",
    "</ol>\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What modeling looks like without Determined\n",
    "\n",
    "<font size=\"4\">First let's look at the kind of work modelers do today.  Below, we train a model we found on Github and modified, printing validation set metrics after each epoch.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE='cuda'\n",
    "#Data loading code\n",
    "device = torch.device(DEVICE)\n",
    "cpu_device = torch.device(DEVICE)\n",
    "print(\"Loading data\")\n",
    "TRAIN_DATA_DIR='determined-ai-xview-coco-dataset/train_sliced_no_neg/train_images_300_02/'\n",
    "VAL_DATA_DIR='determined-ai-xview-coco-dataset/val_sliced_no_neg/val_images_300_02/'\n",
    "\n",
    "dataset, num_classes, dataset_test,data_loader, data_loader_test= load_dataset(TRAIN_DATA_DIR=TRAIN_DATA_DIR,VAL_DATA_DIR=VAL_DATA_DIR,train_batch_size=8,test_batch_size=8)\n",
    "print(\"Create Model\")\n",
    "model = build_frcnn_model(dataset.num_classes)\n",
    "model.to(device)\n",
    "\n",
    "optimizer = torch.optim.SGD(\n",
    "        model.parameters(),\n",
    "        lr=0.01,\n",
    "        momentum=0.9,\n",
    "        weight_decay=1e-4,\n",
    "        nesterov=\"nesterov\",\n",
    "    )\n",
    "\n",
    "scheduler_cls = WarmupWrapper(MultiStepLR)\n",
    "scheduler = scheduler_cls(\n",
    "    'linear',  # warmup schedule\n",
    "    100,  # warmup_iters\n",
    "    0.001,  # warmup_ratio\n",
    "    optimizer,\n",
    "    [177429, 236572],  # milestones\n",
    "    0.1,  # gamma\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Start training\")\n",
    "start_time = time.time()\n",
    "\n",
    "losses, model = train_and_eval(model,data_loader,data_loader_test,optimizer,scheduler,device,cpu_device,epochs=1)\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "total_time_str = str(datetime.timedelta(seconds=int(total_time)))\n",
    "print(f\"Training time {total_time_str}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"4\">We might also roll our own simple hyperparameter tuning:</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "def hp_grid_search():\n",
    "    for lr in np.logspace(-4, -2, num=10):\n",
    "        for m in np.linspace(0.7, 0.95, num=10):\n",
    "            print(f\"Training model with learning rate {lr} and momentum {m}\")\n",
    "            losses, model = train_and_eval(model,data_loader,data_loader_test,optimizer,scheduler,device,cpu_device,epochs=1)\n",
    "\n",
    "\n",
    "try:\n",
    "    hp_grid_search()\n",
    "except KeyboardInterrupt:\n",
    "    pass\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What's Missing?\n",
    "\n",
    "<font size=\"4\">This approach works in theory -- we could get a good model, save it, and use it for predictions.  But we're missing a lot from the ideal state:</font>\n",
    "<font size=\"4\">\n",
    "<ul style=\"margin-top: 15px\">\n",
    "  <li style=\"margin-bottom: 10px\">Distributed training</li>\n",
    "  <li style=\"margin-bottom: 10px\">Parallel search</li>\n",
    "  <li style=\"margin-bottom: 10px\">Intelligent checkpointing</li>\n",
    "  <li style=\"margin-bottom: 10px\">Interruptibility and fault tolerance</li>\n",
    "  <li                            >Logging of experiment configurations and results </li>\n",
    "</ul>\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=6><b>Scaled Experimentation with Determined</b></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With less work than setting up a limited random search, you can get started with Determined."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our First Experiment\n",
    "\n",
    "For our first example, we run a simple single-GPU training job with fixed hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/determined-ai/public_assets/main/images/StartAnExperiment.png\" align=left width=330/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!det e create const-distributed.yaml .."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And evaluate its performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_id = 666"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = Determined().get_experiment(experiment_id).top_checkpoint(sort_by='mAP')\n",
    "checkpoint.uuid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: for this example, need to download checkpoint ahead of time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=build_frcnn_model(num_classes=61)\n",
    "ckpt = torch.load(f'/run/determined/workdir/xview-torchvision-coco/checkpoints/{checkpoint.uuid}/state_dict.pth',map_location='cpu')\n",
    "m_ckpt = load_determined_state_dict(ckpt)\n",
    "# print(m_ckpt)\n",
    "model.load_state_dict(m_ckpt)\n",
    "model.to('cuda')\n",
    "model.eval()\n",
    "print(\"Done Loading\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the prediction of the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(10):\n",
    "    images_t, targets_t= next(iter(data_loader_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# images_t[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = predict(model,images_t,targets_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference on Large Imagery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Imagey(\"/run/determined/workdir/1065.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycocotools.coco import COCO\n",
    "coco = COCO('/run/determined/workdir/val.json')\n",
    "cat_mapping = {0:'background'}\n",
    "cat_mapping.update({str(int(i['id'])+1):i['name'] for i in coco.cats.values()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from terminaltables import AsciiTable\n",
    "\n",
    "def report_objects_detected(result):\n",
    "    '''\n",
    "    result: sahi.PredictionResult\n",
    "    '''\n",
    "    names = []\n",
    "    for r in result.object_prediction_list:\n",
    "        # print(r.bbox.to_xyxy())\n",
    "        # print(r.score.value)\n",
    "        # print(r.category.id)\n",
    "        # print(r.category.name)\n",
    "        names.append(r.category.name)\n",
    "    resulting_objects_detected = list(dict(Counter(names)).items())\n",
    "    resulting_objects_detected.insert(0,('Object Categories','Number of Objects Detected'))\n",
    "    print(AsciiTable(resulting_objects_detected).table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detection_model = AutoDetectionModel.from_pretrained(\n",
    "    model_type='torchvision',\n",
    "    model=model,\n",
    "    confidence_threshold=0.5,\n",
    "    image_size=800,\n",
    "    device=\"cuda\", # or \"cuda:0\"\n",
    "    load_at_init=True,\n",
    "    category_mapping=cat_mapping\n",
    ")\n",
    "result = get_sliced_prediction(\n",
    "    \"/run/determined/workdir/1065.png\",\n",
    "    detection_model,\n",
    "    slice_height = 320,\n",
    "    slice_width = 320,\n",
    "    overlap_height_ratio = 0.2,\n",
    "    overlap_width_ratio = 0.2,\n",
    "    verbose=2\n",
    ")\n",
    "report_objects_detected(result)\n",
    "result.export_visuals(export_dir=\"/run/determined/workdir/\")\n",
    "# Imagey(\"/run/determined/workdir/prediction_visual.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare to Ground Truth annotations, see strong recall and precision with model predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load ground truth coco annotations\n",
    "\n",
    "coco = COCO('/run/determined/workdir/val.json')\n",
    "cat_mapping = {0:'background'}\n",
    "cat_mapping.update({str(int(i['id'])+1):i['name'] for i in coco.cats.values()})\n",
    "# coco.imgs[1]\n",
    "# get specific annotation ids\n",
    "ann_ids = coco.getAnnIds([1])\n",
    "anns = [coco.anns[a] for a in ann_ids]\n",
    "from PIL import Image, ImageDraw\n",
    "# im = Image.open('/run/determined/workdir/prediction_visual.png')\n",
    "im = Image.open('/run/determined/workdir/1065.png')\n",
    "draw = ImageDraw.Draw(im)\n",
    "names_gt = []\n",
    "for a in anns:\n",
    "    x,y,w,h = a['bbox']\n",
    "    l = a['category_id']\n",
    "    name = cat_mapping[str(l)]\n",
    "    names_gt.append(name)\n",
    "    x2 = x+w\n",
    "    y2 = y+h\n",
    "    # print(x,y,x2,y2,l)\n",
    "    draw.rectangle([x,y,x2,y2],outline=(255,255,255),fill=None,width=2)\n",
    "    draw.text([x,y-10],\"{}:{}\".format(l,name))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resulting_objects_detected = list(dict(Counter(names_gt)).items())\n",
    "resulting_objects_detected.insert(0,('Object Categories','Number of GT Objects in Image'))\n",
    "print(AsciiTable(resulting_objects_detected).table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict(model, 'test.jpg', 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling up to Distributed Training\n",
    "\n",
    "Determined makes it trivial to move from single-GPU to multi-GPU (and even multi-node) training. Here we'll simply modify the config above to request 8 GPUs instead of 1, and increase the global batch size to increase the data throughput "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat const-distributed.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!det experiment create distributed.yaml ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/determined-ai/public_assets/main/images/4GPUexperiment.png\" align=left width=530 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Distributed Hyperparameter Tuning\n",
    "\n",
    "By simply building a config file and adapting our code to meet the determined trial interface, we can conduct a sophisticated hyperparamter search.  Instructions for how to configure different types of experiments [can be found in the Determined documentation.](https://docs.determined.ai/latest/how-to/index.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !cat search.yaml# Andrew(11/2021):TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create your Experiment\n",
    "\n",
    "Now that you've described your experiment, you'll simply need to use the command line interface to submit it to the Determined Cluster.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !det experiment create search.yaml . # Andrew(11/2021):TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/determined-ai/public_assets/main/images/12GPUexperiment.png\" align=left width=800 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Registry\n",
    "\n",
    "After training, we'll want to actually use our model in some sort of system.  Determined provides a model registry to version your trained models, making them easy to retrieve for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_id = 666\n",
    "MODEL_NAME = \"satellite-imagery-detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best checkpoint from the training\n",
    "checkpoint = Determined().get_experiment(experiment_id).top_checkpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = check_model(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.register_version(checkpoint.uuid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference\n",
    "\n",
    "Once your model is versioned in the model registry, using that model for inference is straightforward:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve latest checkpoint for a given model name\n",
    "latest_version = model.get_version()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model checkpoint into memory\n",
    "inference_model = latest_version.checkpoint.load().model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run inference as before\n",
    "predict(inference_model, 'test.jpg')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
