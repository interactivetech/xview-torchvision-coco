{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+\n",
      "| categoy | AP50 |\n",
      "+---------+------+\n",
      "| cat     | 0.01 |\n",
      "| dog     | 0.02 |\n",
      "| mouse   | nan  |\n",
      "+---------+------+\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TORCHVISION_VERSION:  0.13.1+cu102 /opt/conda/lib/python3.8/site-packages/torchvision/__init__.py\n",
      "TORCH_VERSION:  1.12.1+cu102 /opt/conda/lib/python3.8/site-packages/torch/__init__.py\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import torch\n",
    "from utils.data import build_dataset,build_xview_dataset, unwrap_collate_fn\n",
    "from attrdict import AttrDict\n",
    "from utils.group_by_aspect_ratio import create_aspect_ratio_groups, GroupedBatchSampler\n",
    "from utils.fcos import fcos_resnet50_fpn\n",
    "# from torchvision.models.detection import fcos_resnet50_fpn\n",
    "from torchvision.models.detection import ssd300_vgg16\n",
    "\n",
    "import datetime\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "from utils.engine import train_and_eval,eval_model\n",
    "import torchvision\n",
    "from pycocotools import mask as coco_mask\n",
    "from pycocotools.coco import COCO\n",
    "import math\n",
    "from lr_schedulers import WarmupWrapper\n",
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "\n",
    "from utils.model import make_custom_object_detection_model_fcos, build_frcnn_model\n",
    "import matplotlib.pyplot as plt\n",
    "from train import load_dataset\n",
    "import numpy as np\n",
    "\n",
    "from determined.experimental import Determined\n",
    "\n",
    "# remove warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set up .detignore file so the checkpoints directory is not packaged into future experiments\n",
    "!echo checkpoints > .detignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_exp(lr=None,momentum=None,epochs=None):\n",
    "    '''\n",
    "    '''\n",
    "    model = build_frcnn_model(dataset.num_classes)\n",
    "    model.to(device)\n",
    "\n",
    "    optimizer = torch.optim.SGD(\n",
    "            model.parameters(),\n",
    "            lr=lr,\n",
    "            momentum=momentum,\n",
    "            weight_decay=1e-4,\n",
    "            nesterov=\"nesterov\",\n",
    "        )\n",
    "\n",
    "    scheduler_cls = WarmupWrapper(MultiStepLR)\n",
    "    scheduler = scheduler_cls(\n",
    "        'linear',  # warmup schedule\n",
    "        100,  # warmup_iters\n",
    "        0.001,  # warmup_ratio\n",
    "        optimizer,\n",
    "        [177429, 236572],  # milestones\n",
    "        0.1,  # gamma\n",
    "    )\n",
    "    print(\"Start training\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    losses, model = train_and_eval(model,data_loader,data_loader_test,optimizer,scheduler,device,cpu_device,epochs=epochs)\n",
    "\n",
    "    total_time = time.time() - start_time\n",
    "    total_time_str = str(datetime.timedelta(seconds=int(total_time)))\n",
    "    print(f\"Training time {total_time_str}\")\n",
    "\n",
    "def visualize_pred(inv_tensor,res):\n",
    "    '''\n",
    "    '''\n",
    "    img = Image.fromarray((255.*inv_tensor.permute((1,2,0)).numpy()).astype(np.uint8))\n",
    "    draw = ImageDraw.Draw(img)\n",
    "    for ind,(b,s,l) in enumerate(zip(res[1]['boxes'],res[1]['scores'],res[1]['labels'])):\n",
    "        # print(b.detach().numpy(), s.detach().numpy())\n",
    "        x,y,x2,y2 = b.detach().numpy()\n",
    "        print( x,y,x2,y2,s.item(),l.item())\n",
    "        draw.rectangle([x,y,x2,y2])\n",
    "        if ind > 3:\n",
    "            break\n",
    "\n",
    "    plt.imshow(img)\n",
    "def predict(model,images_t):\n",
    "    '''\n",
    "    '''\n",
    "    cpu_device = torch.device('cpu')\n",
    "    outputs = model(images_t)\n",
    "\n",
    "    outputss = []\n",
    "    for t in outputs:\n",
    "        outputss.append({k: v.to(cpu_device) for k, v in t.items()})\n",
    "    # model_time = time.time() - model_time\n",
    "    res = {target[\"image_id\"].item(): output for target, output in zip(targets_t, outputss)}\n",
    "    visualize_pred(images_t,res)\n",
    "    return img, res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/determined-ai/determined/master/determined-logo.png\" align='right' width=150 />\n",
    "\n",
    "# Building a Geospatial Detection Model with Determined\n",
    "\n",
    "<img src=\"https://www.cis.upenn.edu/~jshi/ped_html/images/PennPed00071_1.png\" width=400 />\n",
    "\n",
    "\n",
    "This notebook will walk through the benefits of building a Deep Learning model with Determined.  We will build an object detection model trained on the [Penn-Fudan Database for Pedestrian Detection and Segmentation](https://www.cis.upenn.edu/~jshi/ped_html/).\n",
    "\n",
    "\n",
    "# Table of Contents\n",
    "\n",
    "\n",
    "<font size=\"3\">\n",
    "<ol>\n",
    "  <li>What Modeling looks like Today</li>\n",
    "  <li>Building a model with Determined\n",
    "    <ol>\n",
    "      <li>Single GPU training</li>\n",
    "      <li>Cluster-scale multi-GPU training</li>\n",
    "      <li>Adapative hyperparameter search</li>\n",
    "    </ol>\n",
    "  </li>\n",
    "</ol>\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What modeling looks like without Determined\n",
    "\n",
    "<font size=\"4\">First let's look at the kind of work modelers do today.  Below, we train a model we found on Github and modified, printing validation set metrics after each epoch.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data\n",
      "PATHS:  {'train': ('determined-ai-xview-coco-dataset/train_sliced_no_neg/train_images_300_02/', '/tmp/train_sliced_no_neg/train_300_02_1k.json'), 'val': ('determined-ai-xview-coco-dataset/train_sliced_no_neg/train_images_300_02/', '/tmp/val_sliced_no_neg/val_300_02_1k.json')}\n",
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "self.catIdtoCls:  {0: 1, 1: 2, 2: 3, 3: 4, 4: 5, 5: 6, 6: 7, 7: 8, 8: 9, 9: 10, 10: 11, 11: 12, 12: 13, 13: 14, 14: 15, 15: 16, 16: 17, 17: 18, 18: 19, 19: 20, 20: 21, 21: 22, 22: 23, 23: 24, 24: 25, 25: 26, 26: 27, 27: 28, 28: 29, 29: 30, 30: 31, 31: 32, 32: 33, 33: 34, 34: 35, 35: 36, 36: 37, 37: 38, 38: 39, 39: 40, 40: 41, 41: 42, 42: 43, 43: 44, 44: 45, 45: 46, 46: 47, 47: 48, 48: 49, 49: 50, 50: 51, 51: 52, 52: 53, 53: 54, 54: 55, 55: 56, 56: 57, 57: 58, 58: 59, 59: 60}\n",
      "--num_classes:  61\n",
      "PATHS:  {'train': ('determined-ai-xview-coco-dataset/val_sliced_no_neg/val_images_300_02/', '/tmp/train_sliced_no_neg/train_300_02_1k.json'), 'val': ('determined-ai-xview-coco-dataset/val_sliced_no_neg/val_images_300_02/', '/tmp/val_sliced_no_neg/val_300_02_1k.json')}\n",
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "self.catIdtoCls:  {0: 1, 1: 2, 2: 3, 3: 4, 4: 5, 5: 6, 6: 7, 7: 8, 8: 9, 9: 10, 10: 11, 11: 12, 12: 13, 13: 14, 14: 15, 15: 16, 16: 17, 17: 18, 18: 19, 19: 20, 20: 21, 21: 22, 22: 23, 23: 24, 24: 25, 25: 26, 26: 27, 27: 28, 28: 29, 29: 30, 30: 31, 31: 32, 32: 33, 33: 34, 34: 35, 35: 36, 36: 37, 37: 38, 38: 39, 39: 40, 40: 41, 41: 42, 42: 43, 43: 44, 44: 45, 45: 46, 46: 47, 47: 48, 48: 49, 49: 50, 50: 51, 51: 52, 52: 53, 53: 54, 54: 55, 55: 56, 56: 57, 57: 58, 58: 59, 59: 60}\n",
      "Creating data loaders\n",
      "Create Model\n"
     ]
    }
   ],
   "source": [
    "DEVICE='cuda'\n",
    "#Data loading code\n",
    "device = torch.device(DEVICE)\n",
    "cpu_device = torch.device(DEVICE)\n",
    "print(\"Loading data\")\n",
    "TRAIN_DATA_DIR='determined-ai-xview-coco-dataset/train_sliced_no_neg/train_images_300_02/'\n",
    "VAL_DATA_DIR='determined-ai-xview-coco-dataset/val_sliced_no_neg/val_images_300_02/'\n",
    "\n",
    "dataset, num_classes, dataset_test,data_loader, data_loader_test= load_dataset(TRAIN_DATA_DIR=TRAIN_DATA_DIR,VAL_DATA_DIR=VAL_DATA_DIR,train_batch_size=8,test_batch_size=8)\n",
    "print(\"Create Model\")\n",
    "model = build_frcnn_model(dataset.num_classes)\n",
    "model.to(device)\n",
    "\n",
    "optimizer = torch.optim.SGD(\n",
    "        model.parameters(),\n",
    "        lr=0.01,\n",
    "        momentum=0.9,\n",
    "        weight_decay=1e-4,\n",
    "        nesterov=\"nesterov\",\n",
    "    )\n",
    "\n",
    "scheduler_cls = WarmupWrapper(MultiStepLR)\n",
    "scheduler = scheduler_cls(\n",
    "    'linear',  # warmup schedule\n",
    "    100,  # warmup_iters\n",
    "    0.001,  # warmup_ratio\n",
    "    optimizer,\n",
    "    [177429, 236572],  # milestones\n",
    "    0.1,  # gamma\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training\n",
      "Epochs: 2\n",
      "Epoch #: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [08:15<00:00,  3.96s/it, loss=['loss: 0.027', 'loss_classifier: 0.016', 'loss_box_reg: 0.008', 'loss_objectness: 0.002', 'loss_rpn_box_reg: 0.000']]\n",
      "100%|██████████| 125/125 [03:43<00:00,  1.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accumulating evaluation results...\n",
      "DONE (t=0.01s).\n",
      "IoU metric: bbox\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.231\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.802\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.161\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.231\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.260\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.380\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.380\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.380\n",
      "~~~~ Mean and per-category AP @ IoU=[0.50,0.50] ~~~~\n",
      "80.2\n",
      "Per Class AP:\n",
      "\n",
      "+------------------------+----------+\n",
      "| category               | AP       |\n",
      "+------------------------+----------+\n",
      "| Fixed-wing Aircraft    | nan      |\n",
      "| Small Aircraft         | nan      |\n",
      "| Cargo Plane            | 0.230752 |\n",
      "| Helicopter             | nan      |\n",
      "| Passenger Vehicle      | nan      |\n",
      "| Small Car              | nan      |\n",
      "| Bus                    | nan      |\n",
      "| Pickup Truck           | nan      |\n",
      "| Utility Truck          | nan      |\n",
      "| Truck                  | nan      |\n",
      "| Cargo Truck            | nan      |\n",
      "| Truck w/Box            | nan      |\n",
      "| Truck Tractor          | nan      |\n",
      "| Trailer                | nan      |\n",
      "| Truck w/Flatbed        | nan      |\n",
      "| Truck w/Liquid         | nan      |\n",
      "| Crane Truck            | nan      |\n",
      "| Railway Vehicle        | nan      |\n",
      "| Passenger Car          | nan      |\n",
      "| Cargo Car              | nan      |\n",
      "| Flat Car               | nan      |\n",
      "| Tank car               | nan      |\n",
      "| Locomotive             | nan      |\n",
      "| Maritime Vessel        | nan      |\n",
      "| Motorboat              | nan      |\n",
      "| Sailboat               | nan      |\n",
      "| Tugboat                | nan      |\n",
      "| Barge                  | nan      |\n",
      "| Fishing Vessel         | nan      |\n",
      "| Ferry                  | nan      |\n",
      "| Yacht                  | nan      |\n",
      "| Container Ship         | nan      |\n",
      "| Oil Tanker             | nan      |\n",
      "| Engineering Vehicle    | nan      |\n",
      "| Tower crane            | nan      |\n",
      "| Container Crane        | nan      |\n",
      "| Reach Stacker          | nan      |\n",
      "| Straddle Carrier       | nan      |\n",
      "| Mobile Crane           | nan      |\n",
      "| Dump Truck             | nan      |\n",
      "| Haul Truck             | nan      |\n",
      "| Scraper/Tractor        | nan      |\n",
      "| Front loader/Bulldozer | nan      |\n",
      "| Excavator              | nan      |\n",
      "| Cement Mixer           | nan      |\n",
      "| Ground Grader          | nan      |\n",
      "| Hut/Tent               | nan      |\n",
      "| Shed                   | nan      |\n",
      "| Building               | nan      |\n",
      "| Aircraft Hangar        | nan      |\n",
      "| Damaged Building       | nan      |\n",
      "| Facility               | nan      |\n",
      "| Construction Site      | nan      |\n",
      "| Vehicle Lot            | nan      |\n",
      "| Helipad                | nan      |\n",
      "| Storage Tank           | nan      |\n",
      "| Shipping container lot | nan      |\n",
      "| Shipping Container     | nan      |\n",
      "| Pylon                  | nan      |\n",
      "| Tower                  | nan      |\n",
      "+------------------------+----------+\n",
      "mean coco AP:  0.230752\n",
      "\n",
      "+------------------------+----------+\n",
      "| category               | AP@50    |\n",
      "+------------------------+----------+\n",
      "| Fixed-wing Aircraft    | nan      |\n",
      "| Small Aircraft         | nan      |\n",
      "| Cargo Plane            | nan      |\n",
      "| Helicopter             | 0.801980 |\n",
      "| Passenger Vehicle      | nan      |\n",
      "| Small Car              | nan      |\n",
      "| Bus                    | nan      |\n",
      "| Pickup Truck           | nan      |\n",
      "| Utility Truck          | nan      |\n",
      "| Truck                  | nan      |\n",
      "| Cargo Truck            | nan      |\n",
      "| Truck w/Box            | nan      |\n",
      "| Truck Tractor          | nan      |\n",
      "| Trailer                | nan      |\n",
      "| Truck w/Flatbed        | nan      |\n",
      "| Truck w/Liquid         | nan      |\n",
      "| Crane Truck            | nan      |\n",
      "| Railway Vehicle        | nan      |\n",
      "| Passenger Car          | nan      |\n",
      "| Cargo Car              | nan      |\n",
      "| Flat Car               | nan      |\n",
      "| Tank car               | nan      |\n",
      "| Locomotive             | nan      |\n",
      "| Maritime Vessel        | nan      |\n",
      "| Motorboat              | nan      |\n",
      "| Sailboat               | nan      |\n",
      "| Tugboat                | nan      |\n",
      "| Barge                  | nan      |\n",
      "| Fishing Vessel         | nan      |\n",
      "| Ferry                  | nan      |\n",
      "| Yacht                  | nan      |\n",
      "| Container Ship         | nan      |\n",
      "| Oil Tanker             | nan      |\n",
      "| Engineering Vehicle    | nan      |\n",
      "| Tower crane            | nan      |\n",
      "| Container Crane        | nan      |\n",
      "| Reach Stacker          | nan      |\n",
      "| Straddle Carrier       | nan      |\n",
      "| Mobile Crane           | nan      |\n",
      "| Dump Truck             | nan      |\n",
      "| Haul Truck             | nan      |\n",
      "| Scraper/Tractor        | nan      |\n",
      "| Front loader/Bulldozer | nan      |\n",
      "| Excavator              | nan      |\n",
      "| Cement Mixer           | nan      |\n",
      "| Ground Grader          | nan      |\n",
      "| Hut/Tent               | nan      |\n",
      "| Shed                   | nan      |\n",
      "| Building               | nan      |\n",
      "| Aircraft Hangar        | nan      |\n",
      "| Damaged Building       | nan      |\n",
      "| Facility               | nan      |\n",
      "| Construction Site      | nan      |\n",
      "| Vehicle Lot            | nan      |\n",
      "| Helipad                | nan      |\n",
      "| Storage Tank           | nan      |\n",
      "| Shipping container lot | nan      |\n",
      "| Shipping Container     | nan      |\n",
      "| Pylon                  | nan      |\n",
      "| Tower                  | nan      |\n",
      "+------------------------+----------+\n",
      "mean coco AP@50:  0.80198\n",
      "Epoch #: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 125/125 [08:16<00:00,  3.97s/it, loss=['loss: 0.056', 'loss_classifier: 0.032', 'loss_box_reg: 0.018', 'loss_objectness: 0.003', 'loss_rpn_box_reg: 0.002']]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time 0:20:17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Start training\")\n",
    "start_time = time.time()\n",
    "\n",
    "losses, model = train_and_eval(model,data_loader,data_loader_test,optimizer,scheduler,device,cpu_device,epochs=2)\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "total_time_str = str(datetime.timedelta(seconds=int(total_time)))\n",
    "print(f\"Training time {total_time_str}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"4\">We might also roll our own simple hyperparameter tuning:</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hp_grid_search():\n",
    "    for lr in np.logspace(-4, -2, num=10):\n",
    "        for m in np.linspace(0.7, 0.95, num=10):\n",
    "            print(f\"Training model with learning rate {lr} and momentum {m}\")\n",
    "            define_exp(lr=lr,momentum=m,epochs=1)\n",
    "\n",
    "\n",
    "try:\n",
    "    hp_grid_search()\n",
    "except KeyboardInterrupt:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What's Missing?\n",
    "\n",
    "<font size=\"4\">This approach works in theory -- we could get a good model, save it, and use it for predictions.  But we're missing a lot from the ideal state:</font>\n",
    "<font size=\"4\">\n",
    "<ul style=\"margin-top: 15px\">\n",
    "  <li style=\"margin-bottom: 10px\">Distributed training</li>\n",
    "  <li style=\"margin-bottom: 10px\">Parallel search</li>\n",
    "  <li style=\"margin-bottom: 10px\">Intelligent checkpointing</li>\n",
    "  <li style=\"margin-bottom: 10px\">Interruptibility and fault tolerance</li>\n",
    "  <li                            >Logging of experiment configurations and results </li>\n",
    "</ul>\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=6><b>Scaled Experimentation with Determined</b></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With less work than setting up a limited random search, you can get started with Determined."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our First Experiment\n",
    "\n",
    "For our first example, we run a simple single-GPU training job with fixed hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/determined-ai/public_assets/main/images/StartAnExperiment.png\" align=left width=330/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing files (/run/determined/workdir/xview-torchvision-coco) to send to master... 23.1KB and 6 files \n",
      "Traceback (most recent call last):\n",
      "  File \"/run/determined/pythonuserbase/lib/python3.8/site-packages/determined/cli/cli.py\", line 261, in main\n",
      "    parsed_args.func(parsed_args)\n",
      "  File \"/run/determined/pythonuserbase/lib/python3.8/site-packages/determined/cli/experiment.py\", line 204, in create\n",
      "    submit_experiment(args)\n",
      "  File \"/run/determined/pythonuserbase/lib/python3.8/site-packages/determined/common/api/authentication.py\", line 403, in f\n",
      "    return func(namespace)\n",
      "  File \"/run/determined/pythonuserbase/lib/python3.8/site-packages/determined/cli/experiment.py\", line 136, in submit_experiment\n",
      "    model_context = context.read_legacy_context(args.model_def)\n",
      "  File \"/run/determined/pythonuserbase/lib/python3.8/site-packages/determined/common/context.py\", line 171, in read_legacy_context\n",
      "    return [v1File_to_dict(f) for f in read_v1_context(local_path, limit)]\n",
      "  File \"/run/determined/pythonuserbase/lib/python3.8/site-packages/determined/common/context.py\", line 150, in read_v1_context\n",
      "    raise ValueError(\n",
      "ValueError: Directory '/run/determined/workdir/xview-torchvision-coco' exceeds the maximum allowed size 95.0MB.\n",
      "Consider using a .detignore file to specify that certain files or directories should be omitted from the context directory.\n",
      "\u001b[31mFailed to create experiment\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!det e create const-distributed.yaml ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And evaluate its performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_id = 663\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = Determined().get_experiment(experiment_id).top_checkpoint()\n",
    "model = checkpoint.load().model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the prediction of the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "targets should not be none when in training mode",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [27]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m images_t, targets_t\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(data_loader_test)[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m----> 2\u001b[0m \u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43mimages_t\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [26]\u001b[0m, in \u001b[0;36mpredict\u001b[0;34m(model, images_t)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m     50\u001b[0m cpu_device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 51\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages_t\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     53\u001b[0m outputss \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m outputs:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torchvision/models/detection/generalized_rcnn.py:62\u001b[0m, in \u001b[0;36mGeneralizedRCNN.forward\u001b[0;34m(self, images, targets)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining:\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m targets \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 62\u001b[0m         \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_assert\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtargets should not be none when in training mode\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     64\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m target \u001b[38;5;129;01min\u001b[39;00m targets:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/__init__.py:833\u001b[0m, in \u001b[0;36m_assert\u001b[0;34m(condition, message)\u001b[0m\n\u001b[1;32m    831\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(condition) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mTensor \u001b[38;5;129;01mand\u001b[39;00m has_torch_function((condition,)):\n\u001b[1;32m    832\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(_assert, (condition,), condition, message)\n\u001b[0;32m--> 833\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m condition, message\n",
      "\u001b[0;31mAssertionError\u001b[0m: targets should not be none when in training mode"
     ]
    }
   ],
   "source": [
    "images_t, targets_t= list(data_loader_test)[0]\n",
    "predict(model,images_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict(model, 'test.jpg', 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling up to Distributed Training\n",
    "\n",
    "Determined makes it trivial to move from single-GPU to multi-GPU (and even multi-node) training. Here we'll simply modify the config above to request 8 GPUs instead of 1, and increase the global batch size to increase the data throughput "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# name: resnet_fpn_fcos_coco_dist_warmup_2_agents\n",
      "name: resnet_fpn_fcos_xview_dist_warmup\n",
      "profiling:\n",
      " enabled: true\n",
      " begin_on_batch: 0\n",
      " end_after_batch: null\n",
      "hyperparameters:\n",
      "    # These settings match that for the 150 epoch run provided in the original repo:\n",
      "    #   https://github.com/facebookresearch/detr\n",
      "    lr: 0.01\n",
      "    # lr: 0.02\n",
      "    momentum: 0.9\n",
      "    # global_batch_size: 32\n",
      "    global_batch_size: 16\n",
      "    weight_decay: 1.0e-4\n",
      "    gamma: 0.1\n",
      "    warmup: linear\n",
      "    warmup_iters: 1000\n",
      "    warmup_ratio: 0.001\n",
      "    step1: 177429 # 3 epochs: 3*59143 == 177429\n",
      "    step2: 236572 # 4 epochs: 4*59143 == 236572\n",
      "    # step1: 946288 # 16 epochs: 16*59143 == 946,288\n",
      "    # step2: 1301146 # 22 epochs: 22*59143 == 1,301,146\n",
      "    # model: mv3_fcos\n",
      "    model: resnet_fcos\n",
      "    # Dataset\n",
      "    dataset_file: coco\n",
      "    backend: aws # specifiy the backend you want to use.  one of: gcs, aws, fake, local\n",
      "    data_dir: determined-ai-coco-dataset # bucket name if using gcs or aws, otherwise directory to dataset\n",
      "    masks: false\n",
      "    num_workers: 4\n",
      "\n",
      "    device: cuda\n",
      "environment:\n",
      "    environment_variables:                                                                          \n",
      "        - NCCL_DEBUG=INFO                                                                           \n",
      "        # You may need to modify this to match your network configuration.                          \n",
      "        - NCCL_SOCKET_IFNAME=ens,eth,ib\n",
      "bind_mounts:\n",
      "    - host_path: /tmp\n",
      "      container_path: /data\n",
      "      read_only: false\n",
      "scheduling_unit: 2000\n",
      "min_validation_period:\n",
      "    # epochs: 1\n",
      "    # batches: 59143\n",
      "    batches: 19714\n",
      "    # batches: 5000\n",
      "searcher:\n",
      "  name: single\n",
      "  metric: mAP\n",
      "  smaller_is_better: true\n",
      "  max_length:\n",
      "      # epochs: 26\n",
      "    #   batches: 1537718 # 26*59143 == 1,537,718\n",
      "    # batches: 59143\n",
      "    batches: 295715 # 5*59143 == 295715\n",
      "records_per_epoch: 59143\n",
      "# records_per_epoch: 1600\n",
      "resources:\n",
      "    # slots_per_trial: 16\n",
      "    slots_per_trial: 8\n",
      "    shm_size: 2000000000\n",
      "max_restarts: 0\n",
      "# environment:\n",
      "#   image:\n",
      "#     gpu: determinedai/model-hub-mmdetection:0.19.0-dev0\n",
      "#   environment_variables:\n",
      "#     - OMP_NUM_THREADS=1 # Following pytorch dtrain, this environment variable is set to 1 to avoid overloading the system.\n",
      "# master_host: 192.168.0.13\n",
      "# entrypoint: model_def:ObjectDetectionTrial # for testing locally\n",
      "entrypoint: python3 -m determined.launch.torch_distributed --trial model_def:ObjectDetectionTrial\n"
     ]
    }
   ],
   "source": [
    "!cat const-distributed.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!det experiment create distributed.yaml ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/determined-ai/public_assets/main/images/4GPUexperiment.png\" align=left width=530 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Distributed Hyperparameter Tuning\n",
    "\n",
    "By simply building a config file and adapting our code to meet the determined trial interface, we can conduct a sophisticated hyperparamter search.  Instructions for how to configure different types of experiments [can be found in the Determined documentation.](https://docs.determined.ai/latest/how-to/index.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat search.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create your Experiment\n",
    "\n",
    "Now that you've described your experiment, you'll simply need to use the command line interface to submit it to the Determined Cluster.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!det experiment create search.yaml ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/determined-ai/public_assets/main/images/12GPUexperiment.png\" align=left width=800 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Registry\n",
    "\n",
    "After training, we'll want to actually use our model in some sort of system.  Determined provides a model registry to version your trained models, making them easy to retrieve for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_id = <Enter Experiment ID>\n",
    "MODEL_NAME = \"pedestrian-detection\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best checkpoint from the training\n",
    "checkpoint = Determined().get_experiment(experiment_id).top_checkpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = check_model(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.register_version(checkpoint.uuid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference\n",
    "\n",
    "Once your model is versioned in the model registry, using that model for inference is straightforward:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve latest checkpoint for a given model name\n",
    "latest_version = model.get_version()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model checkpoint into memory\n",
    "inference_model = latest_version.checkpoint.load().model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run inference as before\n",
    "predict(inference_model, 'test.jpg')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
